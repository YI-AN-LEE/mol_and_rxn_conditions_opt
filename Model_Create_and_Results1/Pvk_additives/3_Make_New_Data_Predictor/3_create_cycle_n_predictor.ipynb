{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.metrics import mean_squared_error  \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_model_path = 'Model_Create_and_Results1/Pvk_additives/0_Create_Ground_Truth_Model/pvkadditives/pvk_rfr_size.pkl'\n",
    "\n",
    "with open(size_model_path, 'rb') as f:\n",
    "    rf_regressor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvk_crystal_predict(df: pd.DataFrame, \n",
    "                        pvk_rfr: RandomForestRegressor, \n",
    "                        ):\n",
    "    \n",
    "    # scroe predict\n",
    "    pvk_size_feature_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)','lab_code','ATSC5v', 'AATSC5Z', 'MATS8se']\n",
    "    df_size = df[pvk_size_feature_list]\n",
    "\n",
    "    return pvk_rfr.predict(df_size), df[pvk_size_feature_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_count = ['c2'] #the cycle that the predictor is trained on, if cycle0 for opt then enter c0 etc.\n",
    "finished_cycle = 2\n",
    "\n",
    "methods = ['PSO','ABC']\n",
    "methods = ['PSO/round6','PSO/round7','PSO/round8','PSO/round9','PSO/round10']\n",
    "methods = ['BO/round1', 'BO/round2', 'BO/round3', 'BO/round4','BO/round5','BO/round6', 'BO/round7', 'BO/round8', 'BO/round9',  'BO/round10']\n",
    "methods = ['Random/round1', 'Random/round2', 'Random/round3', 'Random/round4', 'Random/round5','Random/round6', 'Random/round7', 'Random/round8', 'Random/round9', 'Random/round10']\n",
    "\n",
    "methods = ['PSO-re/round1', 'PSO-re/round2', 'PSO-re/round3', 'PSO-re/round4',  'PSO-re/round5',\n",
    "'PSO-re/round6', 'PSO-re/round7', 'PSO-re/round8', 'PSO-re/round9',  'PSO-re/round10']\n",
    "\n",
    "methods = ['ABC_T/round4', 'ABC_T/round5',\n",
    "            'ABC_T/round6', 'ABC_T/round7', 'ABC_T/round8', 'ABC_T/round9', 'ABC_T/round10']\n",
    "            \n",
    "pattern = r\"\\d{8}\\w+_Report\\.csv\"\n",
    "# pvk_size_feature_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)','lab_code','AATS3i','ATSC5Z','AATSC5Z']\n",
    "pvk_size_feature_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)','lab_code','ATSC5v', 'AATSC5Z', 'MATS8se']\n",
    "\n",
    "\n",
    "parent_directory = 'Model_Create_and_Results1/Pvk_additives'\n",
    "preprocessing_for_analysis = os.path.join(parent_directory, '1_Preprocessing_for_Analysis')\n",
    "make_new_data_predictor = os.path.join(parent_directory, '3_Make_New_Data_Predictor')\n",
    "\n",
    "for method in methods:\n",
    "    prediction_folder = os.path.join(preprocessing_for_analysis, method, cycle_count[-1])\n",
    "    for filename in os.listdir(prediction_folder):\n",
    "        if re.match(pattern, filename):\n",
    "\n",
    "            file_path = os.path.join(prediction_folder, filename)\n",
    "            print(file_path)\n",
    "            pred_df = pd.read_csv(file_path)\n",
    "            pred_df = pred_df.rename(columns={\n",
    "            'Reagent1_(ul)': 'Reagent1 (ul)',\n",
    "            'Reagent2_(ul)': 'Reagent2 (ul)',\n",
    "            'Reagent3_(ul)': 'Reagent3 (ul)',\n",
    "            'Reagent4_(ul)': 'Reagent4 (ul)'})\n",
    "            gt_value = pvk_crystal_predict(pred_df, rf_regressor)\n",
    "            \n",
    "            pred_df = pred_df.drop(columns=['Folder Name', 'crystal_size_Std', 'Fitness'])\n",
    "            pred_df.rename(columns={'Smiles': 'SMILES'}, inplace=True)\n",
    "            pred_df['crystal_score'] = 0\n",
    "            pred_df['crystal_size'] = gt_value[0]\n",
    "            pred_df = pred_df.sort_values(by='crystal_size', ascending=False)\n",
    "            pred_df = pred_df[:10]\n",
    "            if len(cycle_count[0]) == 3:\n",
    "                pred_df.to_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][2])+10}_pred.csv'), index=False)\n",
    "                print('>10 csv saved')\n",
    "                df_gt = pd.read_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][2])+10}.csv'))\n",
    "                df_gt = pd.concat([df_gt, pred_df], axis=0)\n",
    "                df_gt.to_csv(os.path.join(make_new_data_predictor,method, f'cycle{int(cycle_count[-1][2]) + 11}.csv'), index=False)\n",
    "            else:\n",
    "                pred_df.to_csv(os.path.join(make_new_data_predictor, method,  f'cycle{int(cycle_count[-1][1])}_pred.csv'), index=False)\n",
    "                print('csv saved')\n",
    "                df_gt = pd.read_csv(os.path.join(make_new_data_predictor, method,f'cycle{int(cycle_count[-1][1])}.csv'))\n",
    "                df_gt = pd.concat([df_gt, pred_df], axis=0)\n",
    "                df_gt.to_csv(os.path.join(make_new_data_predictor,method,f'cycle{int(cycle_count[-1][1]) + 1}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "import test\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"\n",
    "set parameters\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    #'n_estimators': [100, 200, 300],\n",
    "    'n_estimators': [20, 50, 80],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    #'subsample': [0.3, 0.4, 0.5],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    #'colsample_bytree': [0.3, 0.4, 0.5],\n",
    "    #\"reg_lambda\": [10, 100, 500],\n",
    "    #\"min_child_weight\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "generate latent space vectors\n",
    "\"\"\"\n",
    "\n",
    "# get encoder info in X\n",
    "import sys\n",
    "sys.path.append('VAE_model/cpu')\n",
    "from fast_jtnn import *\n",
    "\n",
    "# setting VAE params\n",
    "model_path = \"VAE_model/model.epoch-39\"\n",
    "vocab_path = \"VAE_model/model.epoch-39/smi_vocab-2.txt\"\n",
    "\n",
    "proc_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)', 'lab_code']\n",
    "param_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)', 'lab_code', 'crystal_size']\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 450\n",
    "latent_size = 32\n",
    "process_cond_size = 5\n",
    "depthG = 20\n",
    "depthT = 3\n",
    "#batch_size = 40\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(vocab_path)]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "# Load the model\n",
    "model = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "X = {}\n",
    "Y = {}\n",
    "\n",
    "for method in methods:\n",
    "    # load the latent vectors\n",
    "    train_data = pd.DataFrame(pd.read_csv(os.path.join(make_new_data_predictor, f'{method}/cycle{finished_cycle+1}.csv')))\n",
    "    #test_data = pd.DataFrame(pd.read_csv('/home/ianlee/opt_ian/Model_Create_and_Results1/Pvk_additives/0_Create_Surrogate_Model/cycle0_data_test.csv'))\n",
    "    train_smiles = train_data['SMILES'].tolist()\n",
    "    #test_smiles = test_data['SMILES'].tolist()\n",
    "    latent_train = []\n",
    "    train_param = []\n",
    "    for i in range(len(train_smiles)):\n",
    "        try:\n",
    "            latent = model.encode_latent_mean([train_smiles[i]])\n",
    "            latent = latent.detach().cpu().numpy()\n",
    "            latent_train.append(latent[0])\n",
    "            train_param.append(train_data.iloc[i][param_list].values)\n",
    "        except:\n",
    "            print(train_smiles[i], 'with process conditions and crystal size', train_data.iloc[i][param_list].values, 'failed to encode')\n",
    "            \n",
    "    latent_train = np.array(latent_train)\n",
    "    print(latent_train.shape)\n",
    "    #latent_test = model.encode_latent_mean(test_smiles)\n",
    "    #latent_test = latent_test.detach().cpu().numpy()\n",
    "\n",
    "    train_param = np.array(train_param)\n",
    "    train_proc = train_param[:, :5]\n",
    "    train_X = np.concatenate((latent_train, train_proc), axis=1)\n",
    "    print(train_X.shape)\n",
    "    crystal_size = train_param[:, 5]\n",
    "\n",
    "    X[method] = train_X\n",
    "    Y[method] = crystal_size\n",
    "\n",
    "    #train_proc = train_data[proc_list].values\n",
    "    #test_proc = test_data[proc_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train the XGBoost model\n",
    "\"\"\"\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    xgb_folder = os.path.join(make_new_data_predictor, f'{method}/cycle{finished_cycle+1}')\n",
    "\n",
    "    #print(np.shape(latent_train),np.shape(train_proc))\n",
    "    X_train, y_train = X[method], Y[method]\n",
    "    #X_train, y_train = np.concatenate((latent_train, train_proc), axis=1), crystal_size\n",
    "    #X_test, y_test = np.concatenate((latent_test, test_proc), axis=1), test_data['crystal_size'].values\n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror')\n",
    "    grid_search = GridSearchCV(model_xgb, parameters, cv=5, scoring='neg_mean_squared_error') #5-fold cross validation\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_xgb.fit(X_train, y_train) #10-fold cross validation\n",
    "                \n",
    "    y_pred = model_xgb.predict(X_train)\n",
    "    #test_pred = model_xgb.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    r2 = r2_score(y_train, y_pred)\n",
    "    #r2_test = r2_score(y_test, test_pred)\n",
    "\n",
    "    mean = np.mean(y_train)  \n",
    "    std = np.std(y_train)  \n",
    "    val = cross_val_score(model_xgb, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_train, y_pred, alpha=0.3)\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--')\n",
    "    plt.xlabel('Ground Truth (mm)', fontsize = 15)\n",
    "    plt.ylabel('Prediction (mm)', fontsize = 15)\n",
    "    plt.title(\n",
    "    f\"Parity Plot\\nR² = {r2:.4f}, Mean CV (R²) = {np.mean(val):.4f}\",\n",
    "    fontsize=16)\n",
    "    plt.savefig(os.path.join(make_new_data_predictor, f'{method}/c{finished_cycle+1}.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print('XGBRegressor')\n",
    "    print(\"MSE between Train and Prediction: \", mse)\n",
    "    print(\"R2 between Train and Prediction: \", r2)\n",
    "    #print(\"R2 between Test and Prediction: \", r2_test)\n",
    "    print(\"Mean of Training Data y: \", mean)\n",
    "    print(\"Std Dev of Training Data y: \", std)\n",
    "    print(\"Cross Validation Score: \", val)\n",
    "    print(\"Mean Corss Validation Score: \", np.mean(val))\n",
    "\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results = results.sort_values(\"mean_test_score\", ascending=False)\n",
    "    results = results[[\"mean_test_score\", \"params\"]]\n",
    "    results = results[:10]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i, row in results.iterrows():\n",
    "        # 取得超參組合\n",
    "        hyp = row[1]\n",
    "\n",
    "        # 訓練模型\n",
    "        model = XGBRegressor(**hyp)\n",
    "        model.fit(X_train, y_train)\n",
    "                #eval_set=[(X_test, y_test)],\n",
    "            # early_stopping_rounds=5)\n",
    "\n",
    "        # 儲存模型\n",
    "        with open(os.path.join(xgb_folder,f\"pvk_size_xgboost{count}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leveler2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_model_path = 'Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/ground_truth_model/Ground_Truth_RandomForrest_Mordred.pkl'\n",
    "\n",
    "with open(yield_model_path, 'rb') as f:\n",
    "    rf_regressor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare Mordred feature for solvent and bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feature list\n",
    "\n",
    "feature_df = pd.read_csv('Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/ground_truth_model/GT_feature_Mordred.csv')['features']\n",
    "solvent_mordred = pd.read_csv('Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/data/solvent_mordred.csv')\n",
    "base_mordred = pd.read_csv('Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/data/base_mordred.csv')\n",
    "\n",
    "solvent_columns = list(set(feature_df).intersection(solvent_mordred))\n",
    "solvent_columns.append('solvent_SMILES')\n",
    "base_columns = list(set(feature_df).intersection(base_mordred))\n",
    "base_columns.append('base_SMILES')\n",
    "\n",
    "solvent_ft = solvent_mordred[solvent_columns]\n",
    "base_ft = base_mordred[base_columns]\n",
    "\n",
    "ligand_feature = []\n",
    "for feature in feature_df:\n",
    "    if 'ligand_' in feature:\n",
    "        Mordred = feature.split('ligand_')[1]\n",
    "        ligand_feature.append(Mordred)\n",
    "        \n",
    "feature_list = feature_df.tolist()\n",
    "print(ligand_feature)\n",
    "print(feature_list)\n",
    "print(solvent_ft)\n",
    "print(base_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the GT values and add them to new dataframe\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "\n",
    "def aryl_yield_predict(df: pd.DataFrame, \n",
    "                        aryl_rfr: RandomForestRegressor, \n",
    "                        ):\n",
    "    \n",
    "    df_yield = df[feature_list]\n",
    "    # df_score = df[pvk_score_feature_list]\n",
    "\n",
    "    # return pvk_rfr.predict(df_size), pvk_rfc.predict(df_score), df[pvk_size_feature_list], df[pvk_score_feature_list]\n",
    "    return aryl_rfr.predict(df_yield)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_count = ['c2'] # if finished run batch1, the cyce count is c0 for it used the cycle0 surrogate model\n",
    "methods = ['ABC', 'PSO']\n",
    "#methods = ['Random/round1', 'Random/round2', 'Random/round3', 'Random/round4', 'Random/round5','Random/round6','Random/round7', 'Random/round8', 'Random/round9', 'Random/round10']\n",
    "#methods = ['Random/round1', 'Random/round2', 'Random/round3', 'Random/round4', 'Random/round5','Random/round6','Random/round7', 'Random/round8','Random/round10']\n",
    "methods = ['BO/round1', 'BO/round2', 'BO/round3', 'BO/round4', 'BO/round5',\n",
    "           'BO/round6', 'BO/round7', 'BO/round8', 'BO/round9', 'BO/round10']\n",
    "finished_cycle = 2\n",
    "pattern = r\"\\d{8}\\w+_Report\\.csv\"\n",
    "# pvk_size_feature_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)','lab_code','AATS3i','ATSC5Z','AATSC5Z']\n",
    "# pvk_size_feature_list = ['Reagent1 (ul)','Reagent2 (ul)','Reagent3 (ul)','Reagent4 (ul)','lab_code','ATSC5v', 'AATSC5Z', 'MATS8se']\n",
    "\n",
    "\n",
    "parent_directory = 'Model_Create_and_Results1/Direct_ary'\n",
    "preprocessing_for_analysis = os.path.join(parent_directory, '1_Preprocessing_for_Analysis')\n",
    "make_new_data_predictor = os.path.join(parent_directory, '3_Make_New_Data_Predictor')\n",
    "\n",
    "cyclen_list = ['Base_SMILES', 'Ligand_SMILES','Solvent_SMILES','Concentration','Temp_C','yield']\n",
    "\n",
    "for method in methods:\n",
    "    prediction_folder = os.path.join(preprocessing_for_analysis, method, cycle_count[-1])\n",
    "    for filename in os.listdir(prediction_folder):\n",
    "        if re.match(pattern, filename):\n",
    "\n",
    "            file_path = os.path.join(prediction_folder, filename)\n",
    "            print(file_path)\n",
    "            pred_df = pd.read_csv(file_path)\n",
    "            gt_value = aryl_yield_predict(pred_df, rf_regressor)\n",
    "            pred_df.rename(columns={'concentration': 'Concentration',\n",
    "                                    'temperature': 'Temp_C',\n",
    "                                    'solvent_SMILES': 'Solvent_SMILES',\n",
    "                                    'base_SMILES': 'Base_SMILES'},\n",
    "                           inplace=True)\n",
    "            pred_df['yield'] = gt_value\n",
    "            pred_df = pred_df[cyclen_list]\n",
    "            pred_df = pred_df.sort_values(by='yield', ascending=False)\n",
    "\n",
    "            if len(cycle_count[0]) == 3:\n",
    "                pred_df.to_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][2])+10}_pred.csv'), index=False)\n",
    "                print('>10 csv saved')\n",
    "                df_gt = pd.read_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][2])+10}.csv'))\n",
    "                df_gt = pd.concat([df_gt, pred_df], axis=0)\n",
    "                df_gt.to_csv(os.path.join(make_new_data_predictor,method, f'cycle{int(cycle_count[-1][2]) + 11}.csv'), index=False)\n",
    "            else:\n",
    "                pred_df.to_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][1])}_pred.csv'), index=False)\n",
    "                print('csv saved')\n",
    "                df_gt = pd.read_csv(os.path.join(make_new_data_predictor, method, f'cycle{int(cycle_count[-1][1])}.csv'))\n",
    "                df_gt = pd.concat([df_gt, pred_df], axis=0)\n",
    "                df_gt.to_csv(os.path.join(make_new_data_predictor,method, f'cycle{int(cycle_count[-1][1]) + 1}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvent_mordred = pd.read_csv('Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/data/solvent_mordred.csv')\n",
    "base_mordred = pd.read_csv('Model_Create_and_Results1/Direct_ary/0_Create_Ground_Truth_Model/data/base_mordred.csv')\n",
    "\n",
    "# prepare the expt + gt data befoe, read differenrt every time\n",
    "# remeber to change the cycle number !!!!!!!!!!\n",
    "# cyclen_data = pd.read_csv('/home/ianlee/opt_ian/Model_Create_and_Results1/Direct_ary/3_Make_New_Data_Predictor/PSO/cycle7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encoder info in X\n",
    "import sys\n",
    "sys.path.append('VAE_model/cpu')\n",
    "from fast_jtnn import *\n",
    "from rdkit import Chem\n",
    "\n",
    "# setting VAE params\n",
    "VAE_path = \"VAE_model/model.epoch-39\"\n",
    "vocab_path = \"VAE_model/model.epoch-39/smi_vocab-2.txt\"\n",
    "\n",
    "vocab_list = [x.strip(\"\\r\\n \") for x in open(vocab_path)]\n",
    "vocab = Vocab(vocab_list)\n",
    "\n",
    "hidden_size = 450\n",
    "latent_size = 32\n",
    "depthT = 20\n",
    "depthG = 3\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "VAE = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG)\n",
    "VAE.load_state_dict(torch.load(VAE_path,map_location=(device)))\n",
    "VAE.cpu()\n",
    "VAE.eval()\n",
    "\n",
    "X = {}\n",
    "Y = {}\n",
    "\n",
    "for method in methods:\n",
    "    #obtain SMILES and latent vector\n",
    "    cyclen_data = pd.DataFrame(pd.read_csv(os.path.join(make_new_data_predictor, f'{method}/cycle{finished_cycle+1}.csv')))\n",
    "\n",
    "    merged_data = pd.merge(cyclen_data, base_mordred, left_on='Base_SMILES', right_on='base_SMILES', how='left')\n",
    "    merged_data = pd.merge(merged_data, solvent_mordred, left_on='Solvent_SMILES', right_on='solvent_SMILES', how='left')\n",
    "\n",
    "\n",
    "    merged_data = merged_data.drop(columns=['Base_SMILES', 'Solvent_SMILES','base_SMILES', 'solvent_SMILES','Unnamed: 0', 'yield'])\n",
    "\n",
    " \n",
    "    \n",
    "    data_smi = list(merged_data['Ligand_SMILES'])\n",
    "        \n",
    "    #print(data_smi)\n",
    "\n",
    "    latent = VAE.encode_latent_mean(data_smi)\n",
    "    latent = latent.detach().cpu().numpy() #latent is a huge numpy array which need to be concat with features\n",
    "\n",
    "\n",
    "    merged_data = merged_data.drop(columns=['Ligand_SMILES'])\n",
    "\n",
    "    # We need to assume that we do not know anything about the solvent and base -- cannot filter the feautres\n",
    "    # X_train = np.concatenate((latent, merged_data[predictor_feature['Feature']]), axis=1)\n",
    "    X[method] = np.concatenate((latent, merged_data), axis=1)\n",
    "    print(X[method].shape)\n",
    "    Y[method] = cyclen_data['yield']\n",
    "    # total = merged_data[predictor_feature['Feature'].tolist() + ['yield']]\n",
    "    # total.to_csv('total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for method in methods:\n",
    "    # assign the new folder here\n",
    "    xgb_folder = os.path.join(make_new_data_predictor, f'{method}/cycle{finished_cycle+1}')\n",
    "\n",
    "    parameters = {\n",
    "        'n_estimators': [90, 100, 110],\n",
    "        'max_depth': [5, 6, 7],\n",
    "        'learning_rate': [0.2, 0.3, 0.4],\n",
    "        'subsample': [0.8, 0.9, 1.0],  \n",
    "\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    grid_search = GridSearchCV(model, parameters, cv=5)\n",
    "    grid_search.fit(X[method], Y[method])\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X[method])\n",
    "    mse = mean_squared_error(Y[method], y_pred)\n",
    "    mean = np.mean(Y[method])  \n",
    "    std = np.std(Y[method])\n",
    "\n",
    "    r2_scores = cross_val_score(model, X[method], y_pred, cv=5, scoring='r2')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(Y[method], y_pred, alpha=0.3)\n",
    "    plt.plot([Y[method].min(), Y[method].max()], [Y[method].min(), Y[method].max()], 'k--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Parity plot')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"MSE between Train and Prediction: \", mse)\n",
    "    print(\"Mean of Training Data y: \", mean)\n",
    "    print(\"Std Dev of Training Data y: \", std)\n",
    "    print(\"cross validation r2 scores: \", r2_scores)\n",
    "    print(\"mean cross validation r2 scores: \", np.mean(r2_scores))\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results.to_csv('opt_result.csv')\n",
    "    results = results.sort_values(\"mean_test_score\", ascending=False)\n",
    "    results = results[[\"mean_test_score\", \"params\"]]\n",
    "    results = results[:10]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, row in results.iterrows():\n",
    "        hyp = row[1]\n",
    "\n",
    "        xgb = XGBRegressor(**hyp)\n",
    "        xgb.fit(X[method], Y[method])\n",
    "\n",
    "        with open(os.path.join(xgb_folder,f\"ary_yield_xgboost{count}_c{int(cycle_count[-1][1])}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(xgb, f)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leveler2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
